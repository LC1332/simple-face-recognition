{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T04:09:03.289433Z",
     "start_time": "2024-03-29T04:09:02.724604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b45870",
   "metadata": {},
   "source": [
    "# Question\n",
    "人脸特征怎么得到的 openai clip √\n",
    "各种信息都有 \n",
    "是不是还需要一个本来耦合的信息 √\n",
    "onlineEvalOnBatch的实现方式\n",
    "\n",
    "\n",
    "可以不用one clip\n",
    "768可能比one 更合适\n",
    "最合适的是RAF（表情分类loss，关键点loss，没有idloss、分类，人脸特征重构loss是有的。）\n",
    "768 表情分类的loss没有，其他的都有\n",
    "# 为什么要给我one clip\n",
    "为了见到每一个人，做测试\n",
    "只有关键点和重构loss\n",
    "# 短期目标\n",
    "*从openai clip 算一个特征 做表情特征检索更靠谱。*\n",
    "# 长期目标\n",
    "从图片出发的128位特征\n",
    "\n",
    "出来的prompt可能不全是表情prompt，embedding进行清理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ed1dc295a529e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T04:09:03.750558Z",
     "start_time": "2024-03-29T04:09:03.290442Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 耦合特征 \n",
    "# lda_model 出来的是人脸识别的特征 其实也带表情\n",
    "# one_clip = pandas.read_parquet(\"one_clip_per_identity.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# 处理celeb_name变为数字\n",
    "# celeb_name = one_clip[\"celeb_name\"].unique()\n",
    "# celeb_name_dict = {name: i for i, name in enumerate(celeb_name)}\n",
    "# one_clip[\"celeb_name\"] = one_clip[\"celeb_name\"].map(celeb_name_dict)\n",
    "# one_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b83f8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定numpy torch 生成的随机数\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 129\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac540bc4546900f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T04:09:03.812246Z",
     "start_time": "2024-03-29T04:09:03.751197Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 关于表情耦合特征\n",
    "clip = pandas.read_parquet(\"RAF_clip_openai.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# image_name\tlabel\tfeature\n",
    "# 处理label=label-1\n",
    "clip[\"label\"] = clip[\"label\"] - 1\n",
    "# 采样70%的数据作为训练集\n",
    "RAF_clip_train = clip.sample(frac=0.7)\n",
    "# 剩下的数据作为测试集\n",
    "RAF_clip_test = clip.drop(RAF_clip_train.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55a83053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载768数据集\n",
    "clip_768 = pandas.read_parquet(\"top_768_identity.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# 处理celeb_name变为数字\n",
    "celeb_name = clip_768[\"celeb_name\"].unique()\n",
    "celeb_name_dict = {name: i for i, name in enumerate(celeb_name)}\n",
    "clip_768[\"label\"] = clip_768[\"celeb_name\"].map(celeb_name_dict)\n",
    "\n",
    "# 采样70%的数据作为训练集\n",
    "clip_768_train = clip_768.sample(frac=0.7)\n",
    "# 剩下的数据作为测试集\n",
    "clip_768_test = clip_768.drop(clip_768_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e10a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class DecoupleDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, clip: pandas.DataFrame):\n",
    "        self.feature = clip[\"feature\"].tolist()\n",
    "        self.label = clip[\"label\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.feature)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 模型输入\n",
    "        # x, identity_label=None, expression_label=None\n",
    "        return self.feature[idx], self.label[idx]\n",
    "\n",
    "    def collate_fn(self, batch: list[tuple[np.ndarray, int, np.ndarray, int]]):\n",
    "        # 从batch中提取出数据和标签\n",
    "        feature, label = zip(*batch)\n",
    "        # 转换为tensor\n",
    "        feature = torch.tensor(feature, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return feature, label\n",
    "\n",
    "\n",
    "class Dataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset: DecoupleDataSet, batch_size=256, **kwargs):\n",
    "        super(Dataloader, self).__init__(\n",
    "            dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6703ccda7f52424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T04:09:04.975574Z",
     "start_time": "2024-03-29T04:09:04.274738Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        #  identity_loss: Literal[\"l1\", \"l2\"] = \"l2\"\n",
    "    ):\n",
    "        super(Loss, self).__init__()\n",
    "        # if identity_loss not in [\"l1\", \"l2\"]:\n",
    "        #     raise ValueError(\"identity_loss must be either 'l1' or 'l2'\")\n",
    "\n",
    "        # self.identity_loss = nn.L1Loss() if identity_loss == \"l1\" else nn.MSELoss()\n",
    "        self.identity_loss = nn.CrossEntropyLoss()\n",
    "        self.expression_loss_1 = nn.CrossEntropyLoss()\n",
    "        self.expression_loss_2 = nn.L1Loss()\n",
    "        self.decouple_loss = self.onlineEvalOnBatch\n",
    "        # L_Decouple =  E_{sample in batch}( ( f_i - E[f_i] ) * (g_j - E[g_j] ) ) （在batch上计算）有5000个这样的值\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        identity_features,\n",
    "        expression_features,\n",
    "        identity_labels=None,\n",
    "        expression_labels=None,\n",
    "    ):\n",
    "\n",
    "        loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        if identity_labels is not None:\n",
    "            identity_labels = identity_labels.cuda()\n",
    "            identity_loss = self.identity_loss(identity_features, identity_labels)\n",
    "            loss += identity_loss\n",
    "            loss_count += 1\n",
    "\n",
    "        if expression_labels is not None:\n",
    "            expression_labels = expression_labels.cuda()\n",
    "            expression_loss_1 = self.expression_loss_1(\n",
    "                expression_features, expression_labels\n",
    "            )\n",
    "\n",
    "            loss += expression_loss_1\n",
    "            loss_count += 1\n",
    "\n",
    "        decouple_loss = self.decouple_loss(identity_features, expression_features)\n",
    "        loss += decouple_loss\n",
    "        loss_count += 1\n",
    "\n",
    "        return loss / loss_count\n",
    "\n",
    "    # 要除\n",
    "    def onlineEvalOnBatch(\n",
    "        self, identity_features: torch.tensor, expression_features: torch.tensor\n",
    "    ):\n",
    "        device = identity_features.device\n",
    "        bs, dim_i = identity_features.shape\n",
    "        bs, dim_e = expression_features.shape\n",
    "\n",
    "        E_i = torch.zeros(dim_i).to(device)\n",
    "        E_e = torch.zeros(dim_e).to(device)\n",
    "\n",
    "        avg_on_batch_i = torch.mean(identity_features, dim=0)\n",
    "        avg_on_batch_e = torch.mean(expression_features, dim=0)\n",
    "\n",
    "        decouple_loss = 0\n",
    "\n",
    "        for i in range(dim_i):\n",
    "            length = i + 1\n",
    "            new_avg_on_length = torch.sum(avg_on_batch_i[:length]) / length\n",
    "            E_i[i] = new_avg_on_length * 0.1 + E_i[i - 1] * 0.9\n",
    "\n",
    "        for i in range(dim_e):\n",
    "            length = i + 1\n",
    "            new_avg_on_length = torch.sum(avg_on_batch_e[:length]) / length\n",
    "            E_e[i] = new_avg_on_length * 0.1 + E_e[i - 1] * 0.9\n",
    "\n",
    "        # [dim_i,1] @ [1,dim_e] = [dim_i,dim_e]\n",
    "        decouple_loss = torch.sum(\n",
    "            (avg_on_batch_i - E_i).unsqueeze(1) @ (avg_on_batch_e - E_e).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        return decouple_loss / dim_i / dim_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7d68a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decouple(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Decouple, self).__init__()\n",
    "\n",
    "        # self.out = nn.TransformerEncoderLayer(\n",
    "        #     d_model=input_dim, nhead=8, dim_feedforward=2048\n",
    "        # )\n",
    "        # 多层Transformer\n",
    "        self.out = nn.Sequential(\n",
    "            \n",
    "            nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=input_dim, nhead=8, dim_feedforward=input_dim\n",
    "                ),\n",
    "                num_layers=6,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d5031dfd3bfbb3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T04:09:04.978272Z",
     "start_time": "2024-03-29T04:09:04.976262Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pickle\n",
    "\n",
    "\n",
    "class fooModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, identity_features_dim: int = 512, expression_features_dim: int = 256\n",
    "    ):\n",
    "        super(fooModel, self).__init__()\n",
    "        lda_model_path = hf_hub_download(\n",
    "            repo_id=\"silk-road/simple-face-recognition\",\n",
    "            filename=\"lda_openai_clip_model.pkl\",\n",
    "        )\n",
    "\n",
    "        with open(lda_model_path, \"rb\") as f:\n",
    "            self.lda_model = pickle.load(f)\n",
    "\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 解耦函数\n",
    "        self.fx = Decouple(512, 7)\n",
    "        self.gx = Decouple(512, 7)\n",
    "\n",
    "        self.loss = Loss()\n",
    "\n",
    "        # softmax\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, identity_label=None, expression_label=None):\n",
    "        x = torch.tensor(self.lda_model.transform(x), dtype=torch.float32)\n",
    "        x = x.cuda()\n",
    "        # Normalize\n",
    "        x = (x - x.mean()) / x.std()\n",
    "\n",
    "        # 人脸识别特征分离\n",
    "        identity_features = self.fx(x)\n",
    "        identity_features = self.softmax(identity_features)\n",
    "        identity_features = self.dropout(identity_features)\n",
    "\n",
    "        # 表情识别特征分离\n",
    "        expression_features = self.gx(x)\n",
    "        expression_features = self.softmax(expression_features)\n",
    "        expression_features = self.dropout(expression_features)\n",
    "\n",
    "        if identity_label is not None or expression_label is not None:\n",
    "            return self.loss(\n",
    "                identity_features, expression_features, identity_label, expression_label\n",
    "            )\n",
    "\n",
    "        return identity_features, expression_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54190a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 7]), torch.Size([256, 7]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "\n",
    "class RandomProjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RandomProjection, self).__init__()\n",
    "        lda_model_path = hf_hub_download(\n",
    "            repo_id=\"silk-road/simple-face-recognition\",\n",
    "            filename=\"lda_openai_clip_model.pkl\",\n",
    "        )\n",
    "\n",
    "        with open(lda_model_path, \"rb\") as f:\n",
    "            self.lda_model = pickle.load(f)\n",
    "\n",
    "        self.identity_random_projection = random_projection.GaussianRandomProjection(7)\n",
    "        self.expression_random_projection = random_projection.GaussianRandomProjection(\n",
    "            7\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> tuple[torch.tensor, torch.tensor]:\n",
    "        x = torch.tensor(self.lda_model.transform(x), dtype=torch.float32)\n",
    "        \n",
    "        identity_features: np.array = self.identity_random_projection.fit_transform(x)\n",
    "        expression_features: np.array = self.expression_random_projection.fit_transform(\n",
    "            x\n",
    "        )\n",
    "\n",
    "        return torch.tensor(identity_features, dtype=torch.float32), torch.tensor(\n",
    "            expression_features, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "\n",
    "# 测试RandomProjection\n",
    "x = torch.randn(256, 512)\n",
    "r = RandomProjection()\n",
    "i, e = r(x)\n",
    "i.shape, e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1462c685baac4e5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T04:09:04.980221Z",
     "start_time": "2024-03-29T04:09:04.979006Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/jyh/miniconda3/envs/face/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "100%|██████████| 34/34 [00:02<00:00, 16.10it/s, loss=0.925]\n",
      "100%|██████████| 34/34 [00:01<00:00, 18.93it/s, loss=0.925]\n",
      "100%|██████████| 15/15 [00:10<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression acc on RAF by decoupled identity(f(x)): 11.03%\n",
      "expression acc on RAF by random projection identity: 14.15%\n",
      "expression acc on RAF by decoupled expression(g(x)): 39.58%\n",
      "expression acc on RAF by random projection expression: 15.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model = fooModel().cuda()\n",
    "model.train()\n",
    "\n",
    "# expression\n",
    "RAF_train_dataset = DecoupleDataSet(RAF_clip_train)\n",
    "RAF_train_dataloader = Dataloader(RAF_train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# # identity\n",
    "# train_dataset_768 = DecoupleDataSet(clip_768_train)\n",
    "# train_dataloader_768 = Dataloader(train_dataset_768, batch_size=256, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(2):\n",
    "    t = tqdm(RAF_train_dataloader)\n",
    "    for batch in t:\n",
    "        features, expression_label = batch\n",
    "\n",
    "        loss = model(features, expression_label=expression_label)\n",
    "        t.set_postfix(loss=loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # t = tqdm(train_dataloader_768)\n",
    "    # for batch in t:\n",
    "    #     features, identity_label = batch\n",
    "\n",
    "    #     loss = model(features, identity_label=identity_label)\n",
    "    #     t.set_postfix(loss=loss.item())\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "# torch.save(model.state_dict(), f\"model-{time.time()}.pth\")\n",
    "\n",
    "\n",
    "# expression推理\n",
    "model.eval()\n",
    "\n",
    "RAF_test_dataset = DecoupleDataSet(RAF_clip_test)\n",
    "RAF_test_dataloader = Dataloader(RAF_test_dataset, batch_size=256)\n",
    "\n",
    "t = tqdm(RAF_test_dataloader)\n",
    "\n",
    "fx_error = 0\n",
    "gx_error = 0\n",
    "fx_random_error = 0\n",
    "gx_random_error = 0\n",
    "random_projection = RandomProjection()\n",
    "\n",
    "for batch in t:\n",
    "    with torch.no_grad():\n",
    "        features, expression_label = batch\n",
    "        identity_features, expression_features = model(features)\n",
    "\n",
    "        identity_features = identity_features.cpu()\n",
    "        expression_features = expression_features.cpu()\n",
    "\n",
    "        fx_error += torch.sum(\n",
    "            torch.argmax(identity_features, dim=1) != expression_label\n",
    "        ).item()\n",
    "        gx_error += torch.sum(\n",
    "            torch.argmax(expression_features, dim=1) != expression_label\n",
    "        ).item()\n",
    "\n",
    "        identity_features_random, expression_features_random = random_projection(\n",
    "            features\n",
    "        )\n",
    "\n",
    "        fx_random_error += torch.sum(\n",
    "            torch.argmax(identity_features_random, dim=1) != expression_label\n",
    "        ).item()\n",
    "        gx_random_error += torch.sum(\n",
    "            torch.argmax(expression_features_random, dim=1) != expression_label\n",
    "        ).item()\n",
    "\n",
    "\n",
    "# 输出正确率(取小数点后两位)\n",
    "print(\n",
    "    f\"expression acc on RAF by decoupled identity(f(x)): {(1 - fx_error / len(RAF_clip_test))*100:.2f}%\"\n",
    ")\n",
    "# 输出随机投影正确率(取小数点后两位)\n",
    "print(\n",
    "    f\"expression acc on RAF by random projection identity: {(1 - fx_random_error / len(RAF_clip_test))*100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"expression acc on RAF by decoupled expression(g(x)): {(1 - gx_error / len(RAF_clip_test))*100:.2f}%\"\n",
    ")\n",
    "# 输出随机投影正确率(取小数点后两位)\n",
    "print(\n",
    "    f\"expression acc on RAF by random projection expression: {(1 - gx_random_error / len(RAF_clip_test))*100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7098dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1) (0, 1)\n",
      "(0, 1) (1, 2)\n",
      "(0, 1) (2, 3)\n",
      "(0, 1) (3, 4)\n",
      "(1, 2) (0, 1)\n",
      "(1, 2) (1, 2)\n",
      "(1, 2) (2, 3)\n",
      "(1, 2) (3, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "a=[1,2]\n",
    "b=[1,2,3,4]\n",
    "for aa,bb in product(enumerate(a),enumerate(b)):\n",
    "    print(aa,bb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
